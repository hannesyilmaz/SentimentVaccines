{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ece26f",
   "metadata": {},
   "source": [
    "1- Data Preparation\n",
    "a. Import the CSV files: TwitterPreCovidEraData.CSV and TwitterPostCovidEraData.CSV\n",
    "b. Perform data cleaning: remove duplicates, irrelevant data, URLs, mentions, emojis, and special characters\n",
    "c. Pre-process the text: convert to lowercase, remove punctuation, tokenize, and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cf4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523aa1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r_/j66c0c851qb14zzfdkr7vc6m0000gn/T/ipykernel_70431/3024194690.py:8: DtypeWarning: Columns (9,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  pre_covid_data = pd.read_csv(pre_covid_path, on_bad_lines=\"skip\")\n"
     ]
    }
   ],
   "source": [
    "# Set data directory path\n",
    "data_dir = \"/Users/hannesyilmaz/Desktop/ECDataProject\"\n",
    "\n",
    "# Load CSV files\n",
    "pre_covid_path = os.path.join(data_dir, \"TwitterPreCovidEraData.CSV\")\n",
    "post_covid_path = os.path.join(data_dir, \"TwitterPostCovidEraData.CSV\")\n",
    "\n",
    "pre_covid_data = pd.read_csv(pre_covid_path, on_bad_lines=\"skip\")\n",
    "post_covid_data = pd.read_csv(post_covid_path, on_bad_lines=\"skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c4a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#[^#]*$\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply data cleaning\n",
    "pre_covid_data[\"cleaned_text\"] = pre_covid_data[\"tweet\"].apply(clean_text)\n",
    "post_covid_data[\"cleaned_text\"] = post_covid_data[\"Content\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec17d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing function\n",
    "def preprocess_text(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokenized]\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb92451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hannesyilmaz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hannesyilmaz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/hannesyilmaz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "\n",
    "# Apply text pre-processing\n",
    "pre_covid_data[\"preprocessed_text\"] = pre_covid_data[\"cleaned_text\"].apply(preprocess_text)\n",
    "post_covid_data[\"preprocessed_text\"] = post_covid_data[\"cleaned_text\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f9fd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets and split into training and testing sets\n",
    "all_data = pd.concat([pre_covid_data, post_covid_data])\n",
    "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save pre-processed data to pickle files\n",
    "train_data.to_pickle(os.path.join(data_dir, \"train_data.pkl\"))\n",
    "test_data.to_pickle(os.path.join(data_dir, \"test_data.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfeb4662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "                   id  conversation_id    created_at        date      time  \\\n",
      "531931   3.420000e+17     3.420000e+17  1.370360e+12  04/06/2013  19:46:15   \n",
      "2189299  1.200000e+18     1.200000e+18  1.570000e+12  25/11/2019  12:50:40   \n",
      "1182246  6.890000e+17     6.890000e+17  1.453010e+12  17/01/2016  09:18:45   \n",
      "1340644  8.100000e+17     8.100000e+17  1.480000e+12  18/12/2016  11:06:36   \n",
      "2143178  1.180000e+18     1.180000e+18  1.570000e+12  11/10/2019  23:08:50   \n",
      "\n",
      "                        timezone       user_id        username  \\\n",
      "531931   E. Africa Standard Time  1.754178e+08      joey_nolan   \n",
      "2189299  E. Africa Standard Time  1.950142e+08    radioshemsfm   \n",
      "1182246  E. Africa Standard Time  2.773298e+09  khalidrafiq138   \n",
      "1340644  E. Africa Standard Time  8.167793e+08        japamarc   \n",
      "2143178  E. Africa Standard Time  1.572479e+09       africanlm   \n",
      "\n",
      "                       name place  ... user_rt_id user_rt retweet_id  \\\n",
      "531931           Joey Nolan   NaN  ...        NaN     NaN        NaN   \n",
      "2189299             ShemsFm   NaN  ...        NaN     NaN        NaN   \n",
      "1182246        khalid rafiq   NaN  ...        NaN     NaN        NaN   \n",
      "1340644           Marc Japa   NaN  ...        NaN     NaN        NaN   \n",
      "2143178  African Leadership   NaN  ...        NaN     NaN        NaN   \n",
      "\n",
      "                                                  reply_to  retweet_date  \\\n",
      "531931   [{'user_id': '175417806', 'username': 'Joey_No...           NaN   \n",
      "2189299  [{'user_id': '195014153', 'username': 'RadioSh...           NaN   \n",
      "1182246  [{'user_id': '2773297820', 'username': 'khalid...           NaN   \n",
      "1340644  [{'user_id': '816779317', 'username': 'JapaMar...           NaN   \n",
      "2143178  [{'user_id': '1572479041', 'username': 'Africa...           NaN   \n",
      "\n",
      "                                              cleaned_text  \\\n",
      "531931    time to shoot up lol with a meningitis vaccin...   \n",
      "2189299  dmarrage de la campagne de rappel de vaccinati...   \n",
      "1182246  blast near pakistan polio vaccination centre i...   \n",
      "1340644  free anti rabies vaccination deworming and vit...   \n",
      "2143178  who anticholera vaccination campaign begins in...   \n",
      "\n",
      "                                         preprocessed_text Name Handle Content  \n",
      "531931   time to shoot up lol with a meningitis vaccina...  NaN    NaN     NaN  \n",
      "2189299  dmarrage de la campagne de rappel de vaccinati...  NaN    NaN     NaN  \n",
      "1182246  blast near pakistan polio vaccination centre i...  NaN    NaN     NaN  \n",
      "1340644  free anti rabies vaccination deworming and vit...  NaN    NaN     NaN  \n",
      "2143178  who anticholera vaccination campaign begin in ...  NaN    NaN     NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "\n",
      "Test data:\n",
      "                   id  conversation_id    created_at        date      time  \\\n",
      "1988610  1.130000e+18     1.130000e+18  1.560000e+12  09/05/2019  20:07:43   \n",
      "1350559  8.190000e+17     8.190000e+17  1.480000e+12  11/01/2017  01:47:00   \n",
      "780733   5.080000e+17     5.080000e+17  1.409870e+12  05/09/2014  01:10:40   \n",
      "1011382  5.890000e+17     5.890000e+17  1.429280e+12  17/04/2015  17:07:38   \n",
      "1626154  9.850000e+17     9.850000e+17  1.520000e+12  13/04/2018  21:22:17   \n",
      "\n",
      "                        timezone       user_id         username  \\\n",
      "1988610  E. Africa Standard Time  1.120953e+08   mitchemracing4   \n",
      "1350559  E. Africa Standard Time  8.886571e+07  reasonable_hank   \n",
      "780733   E. Africa Standard Time  3.098302e+08      philsmith16   \n",
      "1011382  E. Africa Standard Time  1.046006e+08  christian_orlic   \n",
      "1626154  E. Africa Standard Time  8.110000e+17        carmilu68   \n",
      "\n",
      "                    name place  ... user_rt_id user_rt retweet_id  \\\n",
      "1988610     Mark Mitchem   NaN  ...        NaN     NaN        NaN   \n",
      "1350559  reasonable hank   NaN  ...        NaN     NaN        NaN   \n",
      "780733        Phil Smith   NaN  ...        NaN     NaN        NaN   \n",
      "1011382  Christian Orlic   NaN  ...        NaN     NaN        NaN   \n",
      "1626154          Lu Carm   NaN  ...        NaN     NaN        NaN   \n",
      "\n",
      "                                                  reply_to  retweet_date  \\\n",
      "1988610  [{'user_id': '112095289', 'username': 'Mitchem...           NaN   \n",
      "1350559  [{'user_id': '88865710', 'username': 'reasonab...           NaN   \n",
      "780733   [{'user_id': '309830219', 'username': 'PhilSmi...           NaN   \n",
      "1011382  [{'user_id': '104600623', 'username': 'christi...           NaN   \n",
      "1626154  [{'user_id': '811494664259981312', 'username':...           NaN   \n",
      "\n",
      "                                              cleaned_text  \\\n",
      "1988610  mine arebut i can definitely understand why so...   \n",
      "1350559  welcome to antivaccination australia come for ...   \n",
      "780733   badger vaccination scheme launched to curb bov...   \n",
      "1011382  a moving piece on the importance of vaccine de...   \n",
      "1626154  tsk tsk the  poor innocent children  they are ...   \n",
      "\n",
      "                                         preprocessed_text Name Handle Content  \n",
      "1988610  mine arebut i can definitely understand why so...  NaN    NaN     NaN  \n",
      "1350559  welcome to antivaccination australia come for ...  NaN    NaN     NaN  \n",
      "780733   badger vaccination scheme launched to curb bov...  NaN    NaN     NaN  \n",
      "1011382  a moving piece on the importance of vaccine de...  NaN    NaN     NaN  \n",
      "1626154  tsk tsk the poor innocent child they are not i...  NaN    NaN     NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the train_data.pkl and test_data.pkl files\n",
    "train_data_check = pd.read_pickle(os.path.join(data_dir, \"train_data.pkl\"))\n",
    "test_data_check = pd.read_pickle(os.path.join(data_dir, \"test_data.pkl\"))\n",
    "\n",
    "# Print the first 5 rows of the train_data and test_data DataFrames\n",
    "print(\"Train data:\")\n",
    "print(train_data_check.head())\n",
    "\n",
    "print(\"\\nTest data:\")\n",
    "print(test_data_check.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a233177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/hannesyilmaz/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "def label_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)[\"compound\"]\n",
    "    \n",
    "    if sentiment >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "sample_size = 100000\n",
    "train_data_sample = train_data.sample(n=sample_size, random_state=42)\n",
    "test_data_sample = test_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "train_data_sample[\"label\"] = train_data_sample[\"preprocessed_text\"].apply(label_sentiment)\n",
    "test_data_sample[\"label\"] = test_data_sample[\"preprocessed_text\"].apply(label_sentiment)\n",
    "\n",
    "train_data_sample = train_data_sample.dropna(subset=['preprocessed_text'])\n",
    "test_data_sample = test_data_sample.dropna(subset=['preprocessed_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00aa3c",
   "metadata": {},
   "source": [
    "2- Sentiment Analysis Model Selection and Training\n",
    "a. Choose a sentiment analysis model (e.g., Naïve Bayes, Support Vector Machine, LSTM, BERT)\n",
    "b. Train the model on the pre-processed training data\n",
    "c. Evaluate the model using the testing data and adjust hyperparameters as needed\n",
    "d. Repeat steps b and c until the desired performance is achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c61e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58291cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit the vectorizer to the training data\n",
    "X_train = vectorizer.fit_transform(train_data_sample[\"preprocessed_text\"])\n",
    "\n",
    "# Fit the Naïve Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "y_train = train_data_sample[\"label\"]\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cad2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data using the trained vectorizer\n",
    "X_test = vectorizer.transform(test_data_sample[\"preprocessed_text\"])\n",
    "y_test = test_data_sample[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b955627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d3528c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7478\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.82      0.76     31779\n",
      "     neutral       0.90      0.63      0.74     38591\n",
      "    positive       0.67      0.83      0.74     29630\n",
      "\n",
      "    accuracy                           0.75    100000\n",
      "   macro avg       0.76      0.76      0.75    100000\n",
      "weighted avg       0.77      0.75      0.75    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c944cee",
   "metadata": {},
   "source": [
    "3 - Model Application and Analysis\n",
    "a. Applying the trained model to the pre-processed Pre-COVID and Post-COVID datasets\n",
    "b. Calculating sentiment scores for each tweet (e.g., positive, negative, neutral)\n",
    "c. Aggregating the sentiment scores for both datasets to determine overall sentiment (e.g., average score, percentage of positive/negative/neutral tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0dbf2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Application and Analysis\n",
    "\n",
    "# Apply the trained model to the pre-processed Pre-COVID and Post-COVID datasets\n",
    "X_pre_covid = vectorizer.transform(pre_covid_data[\"preprocessed_text\"])\n",
    "X_post_covid = vectorizer.transform(post_covid_data[\"preprocessed_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b327ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment scores for each tweet\n",
    "pre_covid_data[\"sentiment\"] = clf.predict(X_pre_covid)\n",
    "post_covid_data[\"sentiment\"] = clf.predict(X_post_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf8a9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the sentiment scores for both datasets\n",
    "def sentiment_summary(df):\n",
    "    sentiment_counts = df[\"sentiment\"].value_counts(normalize=True) * 100\n",
    "    avg_sentiment = (sentiment_counts[\"positive\"] - sentiment_counts[\"negative\"]) / 100\n",
    "    return sentiment_counts, avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae96692",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_covid_sentiment_counts, pre_covid_avg_sentiment = sentiment_summary(pre_covid_data)\n",
    "post_covid_sentiment_counts, post_covid_avg_sentiment = sentiment_summary(post_covid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7269271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-COVID Sentiment Counts (Percentage):\n",
      " negative    36.589635\n",
      "positive    36.310742\n",
      "neutral     27.099623\n",
      "Name: sentiment, dtype: float64\n",
      "Pre-COVID Average Sentiment: -0.0027889288363032705\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-COVID Sentiment Counts (Percentage):\\n\", pre_covid_sentiment_counts)\n",
    "print(\"Pre-COVID Average Sentiment:\", pre_covid_avg_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "004a3f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-COVID Sentiment Counts (Percentage):\n",
      " positive    73.748309\n",
      "negative    14.208390\n",
      "neutral     12.043302\n",
      "Name: sentiment, dtype: float64\n",
      "Post-COVID Average Sentiment: 0.5953991880920163\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPost-COVID Sentiment Counts (Percentage):\\n\", post_covid_sentiment_counts)\n",
    "print(\"Post-COVID Average Sentiment:\", post_covid_avg_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5637a1",
   "metadata": {},
   "source": [
    "4 - Comparative Analysis\n",
    "a. Compare the overall sentiment scores between the Pre-COVID and Post-COVID datasets\n",
    "b. Identify significant differences in sentiment (e.g., increased/decreased negativity or positivity)\n",
    "c. Visualize the results using bar charts, pie charts, or line charts to illustrate the sentiment differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a5200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
